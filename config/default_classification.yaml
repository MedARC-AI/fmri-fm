# Name of the run. For output directory and wandb.
name: hcp_task_classification

# Prediction task
#   - classification
#   - regression
task: classification

# Description of the run. Goes in wandb notes.
notes: null

output_dir: checkpoints
seed: 7338
start_epoch: 0

# Path to checkpoint to load initial weights.
checkpoint: null

# Whether to do full-finetuning, or frozen probe evaluation.
finetune: false

# Path to checkpoint to resume training.
resume: null

# Model
model: mae_vit_base_patch16

# Standard image size of fmri flat datasets.
# Used when creating the model for setting up the patch embedding and such.
img_size: [224, 560]
in_chans: 1


# Temporal config.
# t_embed_patch_indices and t_pred_patch_indices define which frames from each patch are
# encoded and decoded respectively. The options are:
#   - null (encode/decode all frames in the patch)
#   - a slice expression like '0:8', '0:None:2'
#   - an explicit list of indices
num_frames: 16
t_patch_size: 16
t_embed_patch_indices: null       # encode all frames
t_pred_patch_indices: "0:None:2"  # decode every other frame

# previous option to specify the temporal patch size of decoder, for backwards
# compatibility.
t_pred_patch_size: 1

decoder_depth: 4
sep_pos_embed: true
cls_embed: true

# correct for the number of observed pixels in the patch embedding
mask_patch_embed: false

# Baseline model settings
parcellation_path: datasets/Schaefer2018_400Parcels_7Networks_order.flat.npy

# Classifier settings
# Classifier input pooled embeddings.
representations:
- cls
- avg_patch
- patch

# Intermediate embed dim for attentive probe
attn_pool_embed_dim: null

# Grid of learning rate scale factors and weight decays for linear classifiers.
# Copied from CAPI.
lr_scale_grid: [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]
weight_decay_grid: [5e-4, 1e-3, 5e-2]

# Optimization
epochs: 20
# batch size per gpu, so total_batch_size = batch_size * num_gpus_per_node * num_nodes
batch_size: 128
accum_iter: 1

# absolute_lr = base_lr * total_batch_size / 256
base_lr: 1e-3
# lower lr bound for cyclic schedulers that hit 0
min_lr: 0.0
warmup_epochs: 2
weight_decay: 0.05
beta: [0.9, 0.95]
clip_grad: 1.0
# disable AMP by setting fp32 to true
fp32: false

# Datasets
datasets:
- name: hcp-train
  root: datasets/flat-clips/hcp-train-task-clips-16t
  split_range: [0.0, 0.9]
  shuffle: true

- name: hcp-train-subset
  root: datasets/flat-clips/hcp-train-task-clips-16t
  split_range: [0.0, 0.1]
  shuffle: false

- name: hcp-train-holdout
  root: datasets/flat-clips/hcp-train-task-clips-16t
  split_range: [0.9, 1.0]
  shuffle: false

- name: hcp-val
  root: datasets/flat-clips/hcp-val-task-clips-16t
  shuffle: false

# Target ID map, if not already in the samples.
target_id_map: null
target_key: null

# Number of target classes for classification, or target dimension for regression.
num_classes: 21

# Names of regression targets
target_names: null

# Dataset names for special subsets.
train_dataset_name: hcp-train
val_dataset_name: hcp-train-holdout
test_dataset_name: hcp-val

# Data transform
# clip extreme values to [-vmax, vmax]. note that during flat dataset generation, each
# vertex time series is z-scored for each run. so values should be in normal range.
clip_vmax: 3.0

# normalize each video clip.
#   - global: the clip is globally normalized to mean zero unit variance.
#   - frame: each temporal frame is independently normalized.
# fMRI data have a lot of slow global variation (i.e. the "global signal"). z-scoring
# each clip is one way to try to get the model to focus on more interesting local
# activity variation.
normalize: null

# Data loader
num_workers: 8

checkpoint_period: 1
max_checkpoints: 4

wandb: true
wandb_entity: null
debug: false
device: cuda
