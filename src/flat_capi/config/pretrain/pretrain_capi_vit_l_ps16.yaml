# Name of the run. For output directory and wandb.
# CUDA_VISIBLE_DEVICES=1 python -m flat_capi.main_pretrain --cfg-path src/flat_capi/config/pretrain/capi_vit_l_ps16.yaml
name: pretrain_capi_vit_l_ps16_mae_defaults

# Description of the run. Goes in wandb notes.
notes: vit-l/16 (embed_dim=1024, heads=16, enc=24, dec=4), mask_ratio=0.9

# Root output directory
output_dir: checkpoints

# Seed for reproducibility.
seed: 7338

start_epoch: 0

# Path to checkpoint to resume training.
resume: null

# Model
model: vit_l16_capi
transformers_kwargs:
  embed_dim: 1024
  drop_path_rate: 0.0  # CAPI default: 0.2; set to 0.0 to match MAE base
  block_kwargs:
    attn_kwargs:
      num_heads: 16
encoder_kwargs:
  depth: 24
decoder_kwargs:
  depth: 4  # CAPI default: 12; set to 4 to match MAE base
norm_layer_type: LayerNorm  # CAPI default: RMSNorm; set to LayerNorm to match MAE base
final_norm_kwargs:
  elementwise_affine: true  # CAPI default: false; set true to match MAE base

# Standard image size of fmri flat datasets.
img_size: [224, 560]
in_chans: 1

capi:
  num_clusters: 16384
  student_temp: 0.12
  clustering_kwargs:
    target_temp: 0.06
    pred_temp: 0.12
    n_sk_iter: 3
    bias: true
  clustering_optimizer:
    name: AdamW
    kwargs:
      betas: [0.9, 0.95]
      weight_decay: 0.05
    lr_schedule:
      base_value: 5.0e-4
      final_value: 0.0
      warmup_epochs: 25
      freeze_epochs: 0
      truncate_cos: 1.0

# Temporal/video settings
num_frames: 16
time_as_channels: true
select_frame_index: 0

# Masking/generation params for indices building
mask_ratio: 0.9
prediction_subsampling: 0.05
patch_size: 16

# Optimization
epochs: 250
batch_size: 256
accum_iter: 1
base_lr: 5.0e-4 # scaled acc to effective bs (ref bs=256)
min_lr: 0.0
warmup_epochs: 25
lr_schedule:
  base_value: 5.0e-4 # overrides base_lr
  final_value: 0.0
  warmup_epochs: 25
  freeze_epochs: 0
  truncate_cos: 1.0
weight_decay: 0.1
beta: [0.9, 0.95]
clip_grad: 1.0
precision: bf16

# Optim multipliers
patch_embed_lr_mult: 0.2
rope_lr_mult: 0.0
layernorm_wd_mult: 0.0

# Momentum schedule (EMA)
momentum_schedule:
  start_warmup_value: 1.0
  base_value: 0.999
  final_value: 1.0
  freeze_epochs: 0
  truncate_cos: 1.0
  warmup_epochs: 25

# Datasets (maintain archived paths)
datasets:
  train:
    type: flat-wds
    url: /teamspace/r2_connections/medarc/fmri-fm/datasets/hcp-flat/hcp-flat_{0000..1799}.tar
    clipping: random
    clipping_kwargs:
      oversample: 4.0
    shuffle: true
    samples_per_epoch: 200000

  val_train:
    type: flat-clips
    root: /teamspace/r2_connections/medarc/fmri-fm/datasets/flat-clips/hcp-train-clips-16t
    shuffle: false

  val_hcp:
    type: flat-clips
    root: /teamspace/r2_connections/medarc/fmri-fm/datasets/flat-clips/hcp-val-clips-16t
    shuffle: false

  val_nsd:
    type: flat-clips
    root: /teamspace/r2_connections/medarc/fmri-fm/datasets/flat-clips/nsd-subj01-clips-16t
    shuffle: false

# Data transform (same defaults as MAE, with bbox/masking available via overrides)
clip_vmax: 3.0
normalize: frame
bbox: null
random_crop: true
crop_kwargs:
  scale: [0.6, 1.0]
  ratio: [2.5, 2.5]
  interpolation: 3

# Data loader
num_workers: 16

checkpoint_period: 10
max_checkpoints: null

# Logging cadence/config
log_metrics_unit: step
log_metrics_cadence: 1

wandb: true
wandb_entity: null
wandb_project: fMRI-foundation-model-pretrain
debug: false
device: cuda


