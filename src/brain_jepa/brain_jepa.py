# --------------------------------------------------------
# References:
# I-JEPA: https://github.com/facebookresearch/ijepa
# MAE: https://github.com/facebookresearch/mae
# --------------------------------------------------------

from functools import partial

import torch
import torch.nn as nn

import pandas as pd

import math

# from brain_jepa_vit import VisionTransformer as vit
import brain_jepa.brain_jepa_vit as vit

from brain_jepa.gradient import load_gradient


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def init_model(
    device,
    patch_size=16,
    model_name="vit_base",
    crop_size=224,
    pred_depth=6,
    pred_emb_dim=384,
    gradient_pos_embed=None,
    attn_mode="normal",
    add_w=False,
    gradient_checkpointing=False,
):
    encoder = vit.__dict__[model_name](
        img_size=(crop_size[0], crop_size[1]),
        patch_size=patch_size,
        in_chans=1,
        gradient_pos_embed=gradient_pos_embed,
        attn_mode=attn_mode,
        add_w=add_w,
        gradient_checkpointing=gradient_checkpointing,
    )
    predictor = vit.__dict__["vit_predictor"](
        num_patches=encoder.patch_embed.num_patches,
        num_patches_2d=encoder.patch_embed.num_patches_2d,
        embed_dim=encoder.embed_dim,
        predictor_embed_dim=pred_emb_dim,
        depth=pred_depth,
        num_heads=encoder.num_heads,
        gradient_pos_embed=gradient_pos_embed,
        attn_mode=attn_mode,
        add_w=add_w,
    )

    def init_weights(m):
        if isinstance(m, torch.nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)

    for m in encoder.modules():
        init_weights(m)

    for m in predictor.modules():
        init_weights(m)

    encoder.to(device)
    predictor.to(device)
    return encoder, predictor


class VisionTransformer(nn.Module):
    """Vision Transformer with support for global average pooling"""

    def __init__(
        self,
        model_name="vit_base",
        attn_mode="flash_attn",
        global_pool=False,
        add_w=False,
        device=None,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        embed_dim=None,
        num_classes=2,
        patch_size=16,
        crop_size=(450, 160),
        pred_depth=12,
        pred_emb_dim=384,
    ):
        super(VisionTransformer, self).__init__()

        gradient = load_gradient().to(device, non_blocking=True)

        self.encoder, _ = init_model(
            device=device,
            patch_size=patch_size,
            crop_size=crop_size,
            pred_depth=pred_depth,
            pred_emb_dim=pred_emb_dim,
            model_name=model_name,
            gradient_pos_embed=gradient,
            attn_mode="normal",
            add_w=add_w,
        )
        # gradient_checkpointing=args.gradient_checkpointing)

        # self.gradient_checkpointing = args.gradient_checkpointing

        self.global_pool = global_pool
        if self.global_pool:
            norm_layer = norm_layer
            embed_dim = embed_dim
            self.fc_norm = norm_layer(self.encoder.embed_dim)

        self.head = (
            nn.Linear(self.encoder.embed_dim, num_classes)
            if num_classes > 0
            else nn.Identity()
        )

    @torch.jit.ignore
    def no_weight_decay(self):
        return {"pos_embed", "cls_token"}

    def testing_forward(self, x):
        return self.encoder(x)

    def forward(self, x):
        x = self.encoder(x)
        if self.global_pool:
            x = x[:, :, :].mean(dim=1)  # global pool without cls token
            outcome = self.fc_norm(x)
        else:
            outcome = x[:, 0]

        if self.gradient_checkpointing:
            try:
                x = torch.utils.checkpoint.checkpoint(
                    self.head, outcome, use_reentrant=False
                )
            except ValueError:
                print(1)
        else:
            x = self.head(outcome)

        return x


def vit_base_patch16(**kwargs):
    model = VisionTransformer(
        patch_size=16,
        in_chans=1,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


def vit_large_patch16(**kwargs):
    model = VisionTransformer(
        patch_size=16,
        in_chans=1,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


def vit_huge_patch14(**kwargs):
    model = VisionTransformer(
        patch_size=14,
        in_chans=1,
        embed_dim=1280,
        depth=32,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model
