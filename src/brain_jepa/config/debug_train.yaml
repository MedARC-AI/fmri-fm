# Keep same parameters as original config in Brain JEPA
data:
  batch_size: 16 
  crop_size: 
  - 400
  - 160
  num_workers: 4
  pin_mem: true
  tar_glob: /teamspace/filestore_folders/shared/fmri-fm/datasets/hcp-parc/hcp-parc_*.tar
  gradient_csv_path: src/brain_jepa/gradient_mapping_400.csv
  sampling_rate: 3
  seq_length: 490
logging: 
  folder: logs/debug_train
  write_tag: jepa
mask:
  allow_overlap: false
  patch_size: 16
  min_keep: 4
  enc_mask_scale:
  - 0.84
  - 1 
  pred_mask_R_scale: 
  - 0.45
  - 0.6 
  pred_mask_T_roi_scale:
  - 0.2
  - 0.6
  pred_mask_R_roi_scale:
  - 0.15
  - 0.3
  pred_mask_T_scale:
  - 0.0
  - 0.4

meta:
  load_checkpoint: false
  model_name: vit_base
  pred_depth: 6  
  pred_emb_dim: 384 
  read_checkpoint: null
  use_bfloat16: true
  accumulation_steps: 8
  attn_mode: normal
  add_w: mapping
  downsample: true
  mask_mode: 'roi_mask'
  use_standatdization: false

  # Wandb logging
  use_wandb: true
  wandb_project: 'brain-jepa-pretrain'
  wandb_entity: shamussim
  wandb_run_name: pretrain-lite
optimization:
  ema:
  - 0.996
  - 1.0
  epochs: 300
  final_lr: 1.0e-06
  final_weight_decay: 0.4
  ipe_scale: 1.0
  lr: 0.001
  start_lr: 0.00005 
  warmup: 40
  weight_decay: 0.04